{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edab7bcc",
   "metadata": {},
   "source": [
    "# This notebook contains the LLM-based grading for steering vectors.\n",
    "You will need to enter a Huggingface API key with access to the Llama 3 models. You will also need to provide your Anthropic API key as an environment variable.\n",
    "\n",
    "Note that this is highly unoptimized, and is set up to run on 2x 80gb GPUs (I used 2x A100s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd46f10d-891a-4162-99be-1b381498457a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anthropic\n",
      "  Using cached anthropic-0.43.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.0-cp311-cp311-macosx_10_12_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: datasets in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: einops in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (0.27.1)\n",
      "Collecting jaxtyping\n",
      "  Using cached jaxtyping-0.2.36-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: natsort in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (8.4.0)\n",
      "Requirement already satisfied: simple-parsing in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (0.1.6)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement triton (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for triton\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: safetensors in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (0.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/EleutherAI/sae.git\n",
      "  Cloning https://github.com/EleutherAI/sae.git to /private/var/folders/z3/jj7c77fx7q16vhxjz_nmsh9w0000gn/T/pip-req-build-_uq4ux_6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/EleutherAI/sae.git /private/var/folders/z3/jj7c77fx7q16vhxjz_nmsh9w0000gn/T/pip-req-build-_uq4ux_6\n",
      "  Resolved https://github.com/EleutherAI/sae.git to commit 0b58af41d3da3dcd8df31839612c6dc91c310459\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from sae==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: datasets in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from sae==0.1.0) (3.2.0)\n",
      "Requirement already satisfied: einops in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from sae==0.1.0) (0.8.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from sae==0.1.0) (0.27.1)\n",
      "Requirement already satisfied: natsort in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from sae==0.1.0) (8.4.0)\n",
      "Requirement already satisfied: safetensors in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from sae==0.1.0) (0.5.2)\n",
      "Requirement already satisfied: simple-parsing in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from sae==0.1.0) (0.1.6)\n",
      "Requirement already satisfied: torch in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from sae==0.1.0) (2.2.2)\n",
      "Requirement already satisfied: transformers in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from sae==0.1.0) (4.48.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from accelerate->sae==0.1.0) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from accelerate->sae==0.1.0) (24.2)\n",
      "Requirement already satisfied: psutil in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from accelerate->sae==0.1.0) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from accelerate->sae==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: filelock in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from huggingface-hub->sae==0.1.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from huggingface-hub->sae==0.1.0) (2024.9.0)\n",
      "Requirement already satisfied: requests in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from huggingface-hub->sae==0.1.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from huggingface-hub->sae==0.1.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from huggingface-hub->sae==0.1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from torch->sae==0.1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from torch->sae==0.1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from torch->sae==0.1.0) (3.1.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from datasets->sae==0.1.0) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from datasets->sae==0.1.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from datasets->sae==0.1.0) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from datasets->sae==0.1.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from datasets->sae==0.1.0) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from datasets->sae==0.1.0) (3.11.11)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from simple-parsing->sae==0.1.0) (0.16)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from transformers->sae==0.1.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from transformers->sae==0.1.0) (0.21.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from aiohttp->datasets->sae==0.1.0) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from aiohttp->datasets->sae==0.1.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from aiohttp->datasets->sae==0.1.0) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from aiohttp->datasets->sae==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from aiohttp->datasets->sae==0.1.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from aiohttp->datasets->sae==0.1.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from aiohttp->datasets->sae==0.1.0) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from requests->huggingface-hub->sae==0.1.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from requests->huggingface-hub->sae==0.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from requests->huggingface-hub->sae==0.1.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from requests->huggingface-hub->sae==0.1.0) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from jinja2->torch->sae==0.1.0) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from pandas->datasets->sae==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from pandas->datasets->sae==0.1.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from pandas->datasets->sae==0.1.0) (2024.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from sympy->torch->sae==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/msl/src/steering-research/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->sae==0.1.0) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Notebook / Runpod setup, safe to ignore\n",
    "%pip install anthropic matplotlib accelerate datasets einops huggingface-hub jaxtyping natsort simple-parsing triton transformers gguf sentencepiece scikit-learn seaborn umap-learn\n",
    "%pip install -U safetensors\n",
    "%pip install git+https://github.com/EleutherAI/sae.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0074cbe-195f-43f3-94dd-2a049d99bbbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec2cea5e-f73c-4c67-a3e1-89305878adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import sys, os\n",
    "\n",
    "notebook_dir = os.path.abspath('')  # Get current notebook directory\n",
    "experiments_dir = os.path.dirname(notebook_dir)  # Get parent directory\n",
    "sys.path.insert(0, experiments_dir)\n",
    "os.environ[\"HF_HOME\"] = os.path.join(experiments_dir, \"hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676136dc-cd1e-4448-941c-8dd0e5237f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b55874b65f240b5b200a7f014c41b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be85a3ae-6c14-43b1-8fe2-a98ba9cb0e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/msl/.pyenv/versions/3.11.0/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/msl/.pyenv/versions/3.11.0/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/msl/.pyenv/versions/3.11.0/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/z3/jj7c77fx7q16vhxjz_nmsh9w0000gn/T/ipykernel_85382/4060066235.py\", line 4, in <module>\n",
      "    import torch\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/msl/src/steering-research/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gguf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrepeng\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ControlVector, ControlModel, DatasetEntry\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrepeng\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaes\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/src/steering-research/experiments/repeng/__init__.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, PreTrainedTokenizerBase\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m control, extract\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextract\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ControlVector, DatasetEntry\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ControlModel\n",
      "File \u001b[0;32m~/src/steering-research/experiments/repeng/extract.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgguf\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PCA\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gguf'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from repeng import ControlVector, ControlModel, DatasetEntry\n",
    "import repeng.saes\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import anthropic\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from typing import Union, List, Dict\n",
    "from anthropic.types.beta.message_create_params import MessageCreateParamsNonStreaming\n",
    "from anthropic.types.beta.messages.batch_create_params import Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fc143c3-73d3-43bb-8db1-ef201e06a944",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Force CUDA to sync\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n",
      "File \u001b[0;32m~/src/steering-research/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:799\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msynchronize\u001b[39m(device: _device_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    792\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for all kernels in all streams on a CUDA device to complete.\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \n\u001b[1;32m    794\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m            if :attr:`device` is ``None`` (default).\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m    801\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_synchronize()\n",
      "File \u001b[0;32m~/src/steering-research/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Collect garbage\n",
    "gc.collect()\n",
    "\n",
    "# Force CUDA to sync\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "control_layers = list(range(1, 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "558eadd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d61d8f0920d4382afe291846199803e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e289c5148945d3b3912d74c5f10409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "    torch_dtype=\"auto\").to(\n",
    "    torch.device(\"cuda:0\"))\n",
    "base_model = ControlModel(base_model, control_layers)\n",
    "\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    torch_dtype=\"auto\").to(\n",
    "    torch.device(\"cuda:0\"))\n",
    "instruct_model = ControlModel(instruct_model, control_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe04ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5250c5b5b0cd44218203c894982b6916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 66 files:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "sae_full = repeng.saes.from_eleuther(device=\"cuda:0\", layers=control_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7bd46fb-53a3-4263-a3f2-38be8315575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeng dataloading / template boilerplate\n",
    "with open(os.path.join(notebook_dir, \"data/all_truncated_outputs.json\")) as f:\n",
    "    output_suffixes = json.load(f)\n",
    "truncated_output_suffixes = [\n",
    "    tokenizer.convert_tokens_to_string(tokens[:i])\n",
    "    for tokens in (tokenizer.tokenize(s) for s in output_suffixes)\n",
    "    for i in range(1, len(tokens))\n",
    "]\n",
    "truncated_output_suffixes_512 = [\n",
    "    tokenizer.convert_tokens_to_string(tokens[:i])\n",
    "    for tokens in (tokenizer.tokenize(s) for s in output_suffixes[:512])\n",
    "    for i in range(1, len(tokens))\n",
    "]\n",
    "\n",
    "# base model so roll our own instruct template\n",
    "# TEMPLATE = \"\"\"{persona} is talking to the user.\n",
    "\n",
    "# User: {user_msg}\n",
    "\n",
    "# AI: {prefill}\"\"\"\n",
    "# TEMPLATE = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "# {persona} is talking to the user.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "# {user_msg}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "# {prefill}\"\"\"\n",
    "TEMPLATE = \"\"\"{persona}. {prefill}\"\"\"\n",
    "\n",
    "def template_parse(resp: str) -> tuple[str, str, str]:\n",
    "    if \"<|start_header_id|>user\" in resp:\n",
    "        persona = \"\"\n",
    "        _, rest = resp.split(\"<|start_header_id|>user<|end_header_id|>\")\n",
    "        user, assistant = rest.split(\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\")\n",
    "    elif \"User:\" in resp:\n",
    "        persona, rest = resp.split(\"\\n\\nUser: \", 1)\n",
    "        user, assistant = rest.split(\"\\n\\nAI: \", 1)\n",
    "    else:\n",
    "        persona = \"\"\n",
    "        user = \"\"\n",
    "        assistant = resp\n",
    "\n",
    "    return (persona.strip(), user.strip(), assistant.strip())\n",
    "\n",
    "def make_dataset(\n",
    "    persona_template: str,\n",
    "    positive_personas: list[str],\n",
    "    negative_personas: list[str],\n",
    "    user_msg: str,\n",
    "    suffix_list: list[str]\n",
    ") -> list[DatasetEntry]:\n",
    "    dataset = []\n",
    "    for suffix in suffix_list:\n",
    "        for positive_persona, negative_persona in zip(positive_personas, negative_personas):\n",
    "            pos = persona_template.format(persona=positive_persona)\n",
    "            neg = persona_template.format(persona=negative_persona)\n",
    "            dataset.append(\n",
    "                DatasetEntry(\n",
    "                    positive=TEMPLATE.format(persona=pos, user_msg=user_msg, prefill=suffix),\n",
    "                    negative=TEMPLATE.format(persona=neg, user_msg=user_msg, prefill=suffix),\n",
    "                )\n",
    "            )\n",
    "    return dataset\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from transformers import TextStreamer\n",
    "\n",
    "class HTMLStreamer(TextStreamer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.display_handle = display(display_id=True)\n",
    "        self.full_text = \"\"\n",
    "\n",
    "    def _is_chinese_char(self, _):\n",
    "        # hack to force token-by-token streaming\n",
    "        return True\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.full_text += text\n",
    "        persona, user, assistant = template_parse(self.full_text)\n",
    "        html = HTML(f\"\"\"\n",
    "        <div style='border: 1px solid black; border-radius: 5px; margin-bottom: 5px; padding: 5px;'>\n",
    "            <strong>persona</strong>\n",
    "            <p>{persona}</p>\n",
    "            <strong>user</strong>\n",
    "            <p>{user}</p>\n",
    "            <strong>assistant</strong>\n",
    "            <p>{assistant}</p>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "        self.display_handle.update(html)\n",
    "\n",
    "def generate_with_vector(\n",
    "    model,\n",
    "    input: str,\n",
    "    labeled_vectors: list[tuple[str, ControlVector]],\n",
    "    max_new_tokens: int = 128,\n",
    "    repetition_penalty: float = 1.1,\n",
    "    show_baseline: bool = False,\n",
    "    temperature: float = 0.7,\n",
    "):\n",
    "    input_ids = tokenizer(input, return_tensors=\"pt\").to(model.device)\n",
    "    settings = {\n",
    "        \"pad_token_id\": tokenizer.eos_token_id, # silence warning\n",
    "        #\"do_sample\": False, # temperature=0\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "    }\n",
    "\n",
    "    def gen(label):\n",
    "        display(HTML(f\"<h3>{label}</h3>\"))\n",
    "        output = model.generate(streamer=HTMLStreamer(tokenizer), **input_ids, **settings)\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        # perplexity = calculate_perplexity(model, tokenizer, generated_text)\n",
    "        # display(HTML(f\"<p>Perplexity: {perplexity:.2f}</p>\"))\n",
    "        print(generated_text)\n",
    "\n",
    "    if show_baseline:\n",
    "        model.reset()\n",
    "        gen(\"baseline\")\n",
    "    for label, vector in labeled_vectors:\n",
    "        model.set_control(vector)\n",
    "        gen(label)\n",
    "    model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d038a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_discontinuities(x, y, discontinuities):\n",
    "    \"\"\"\n",
    "    Visualize the detected discontinuities on the original data.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Change from line plot to scatter plot\n",
    "    plt.scatter(x, y, c='b', s=20, alpha=0.6, label='Original data')\n",
    "    \n",
    "    x_discontinuities = x[discontinuities]\n",
    "    y_discontinuities = y[discontinuities]\n",
    "\n",
    "    plt.scatter(x_discontinuities, y_discontinuities, c='r', s=100, \n",
    "               label='Detected discontinuities', zorder=3)\n",
    "    \n",
    "    # Add vertical lines at discontinuities\n",
    "    for idx in discontinuities:\n",
    "        plt.axvline(x=x[idx], color='r', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title('Detected Discontinuities')\n",
    "    plt.show()\n",
    "\n",
    "def find_significant_drop(x, y, drop_threshold=0.10, window_size=1):\n",
    "    \"\"\"\n",
    "    Find the first point where perplexity drops significantly below starting value.\n",
    "    \n",
    "    Parameters:\n",
    "    x: array-like, coefficients\n",
    "    y: array-like, perplexity values\n",
    "    drop_threshold: float, minimum drop as fraction of starting value (default 0.2 = 20%)\n",
    "    window_size: int, number of consecutive points to check to avoid noise (default 3)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (x value where drop occurs, index of drop point)\n",
    "    \"\"\"\n",
    "    # Get baseline from start of sequence\n",
    "    baseline = y[0]\n",
    "    target_value = baseline * (1 + drop_threshold)\n",
    "    \n",
    "    # Look for first window where all values are below target\n",
    "    for i in range(len(y) - window_size + 1):\n",
    "        window = y[i:i+window_size]\n",
    "        if all([val < target_value for val in window]):\n",
    "            return i\n",
    "            \n",
    "    # If no significant drop found\n",
    "    return None\n",
    "\n",
    "def calculate_entropy(model, sequence):\n",
    "    tokens = sequence.tolist()\n",
    "    token_counts = Counter(tokens)\n",
    "    total_tokens = len(tokens)\n",
    "    probabilities = [count / total_tokens for count in token_counts.values()]\n",
    "    entropy = -sum(p * math.log2(p) for p in probabilities if p > 0)\n",
    "    max_entropy = math.log2(len(token_counts))\n",
    "    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0\n",
    "    return normalized_entropy\n",
    "\n",
    "def calculate_perplexity_with_entropy(model, sequence, beta=10):\n",
    "    perplexity = calculate_perplexity(model, sequence)\n",
    "    norm_entropy = calculate_entropy(model, sequence)\n",
    "    adjusted_perplexity = perplexity * (1 + beta * ((1 - norm_entropy)**2))\n",
    "    return adjusted_perplexity\n",
    "\n",
    "def calculate_repetition_score(model, sequence):\n",
    "    \"\"\"\n",
    "    Calculate a repetition score based on repeated token sequences.\n",
    "    Returns a value between 0 and 1, where higher values indicate more repetition.\n",
    "    \"\"\"\n",
    "    tokens = sequence.tolist()\n",
    "    if len(tokens) <= 1:\n",
    "        return 0\n",
    "    \n",
    "    # Look for repeating sequences of 2-4 tokens\n",
    "    ngram_scores = []\n",
    "    for n in range(2, 5):\n",
    "        if len(tokens) > n:\n",
    "            # Create n-grams\n",
    "            ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "            # Count repetitions\n",
    "            ngram_counts = Counter(ngrams)\n",
    "            # Calculate score for this n-gram size\n",
    "            score = sum(c - 1 for c in ngram_counts.values()) / max(len(ngrams), 1)\n",
    "            ngram_scores.append(score)\n",
    "    \n",
    "    # Weight longer sequences more heavily\n",
    "    if ngram_scores:\n",
    "        weights = [1, 2, 3]  # More weight to longer sequences\n",
    "        repetition_score = sum(s * w for s, w in zip(ngram_scores, weights)) / sum(weights)\n",
    "        return min(repetition_score, 1.0)\n",
    "    return 0.0\n",
    "\n",
    "def calculate_perplexity_with_repetition_bias(model, sequence, beta=10):\n",
    "    \"\"\"\n",
    "    Adjust perplexity based on repetition in the token sequence.\n",
    "    Higher repetition leads to higher adjusted perplexity.\n",
    "    \"\"\"\n",
    "    perplexity = calculate_perplexity(model, sequence)\n",
    "    repetition_score = calculate_repetition_score(model, sequence)\n",
    "    adjusted_perplexity = perplexity * (1 + beta * (repetition_score**2))\n",
    "    return adjusted_perplexity\n",
    "\n",
    "def calculate_perplexity(model, sequence):\n",
    "    input_ids = sequence.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss).item()\n",
    "    return perplexity\n",
    "\n",
    "def cross_entropy_loss(model, sequence):\n",
    "    input_ids = sequence.unsqueeze(0)\n",
    "    \n",
    "    # Create targets by shifting input_ids right\n",
    "    targets = input_ids.clone()[:, 1:]  # Remove first token\n",
    "    inputs = input_ids[:, :-1]  # Remove last token\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "    # Use CrossEntropyLoss\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')  # or 'sum' if you want total loss\n",
    "    loss = loss_fn(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def sequence_probability(model, sequence):\n",
    "    input_ids = sequence.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Process tokens in chunks to avoid numerical underflow\n",
    "    chunk_size = 2\n",
    "    inverse_prob = 1.0\n",
    "    \n",
    "    for i in range(0, input_ids.size(1), chunk_size):\n",
    "        chunk_log_prob = 0.0\n",
    "        end_idx = min(i + chunk_size, input_ids.size(1))\n",
    "        \n",
    "        # Get log prob for this chunk\n",
    "        for j in range(i, end_idx):\n",
    "            token_id = input_ids[0, j].item()\n",
    "            token_log_prob = log_probs[0, j, token_id].item()\n",
    "            chunk_log_prob += token_log_prob\n",
    "            \n",
    "        # Convert chunk to probability and multiply running total\n",
    "        chunk_prob = torch.exp(torch.tensor(chunk_log_prob)).item()\n",
    "        inverse_prob *= (1.0 / chunk_prob if chunk_prob > 0 else float('inf'))\n",
    "        \n",
    "        if inverse_prob == float('inf'):\n",
    "            return float('inf')\n",
    "    \n",
    "    return inverse_prob\n",
    "\n",
    "def avg_lob_probs(model, sequence):\n",
    "    input_ids = sequence.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    sequence_prob = 0.0  # Start with 0 since we're adding logs\n",
    "    for i in range(input_ids.size(1)):\n",
    "        token_id = input_ids[0, i].item()\n",
    "        token_log_prob = log_probs[0, i, token_id].item()\n",
    "        sequence_prob += token_log_prob \n",
    "    \n",
    "    return sequence_prob/input_ids.size(1)  # This will be the avg log probability\n",
    "\n",
    "def generate_sequence(model, input_ids, vector, coeff, max_new_tokens, repetition_penalty, temperature):\n",
    "    model.reset()\n",
    "    \n",
    "    model.set_control(coeff * vector)\n",
    "    settings = {\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "    }\n",
    "    with torch.no_grad():  # <-- Add this line\n",
    "        output = model.generate(**input_ids, **settings)\n",
    "    return output[0]\n",
    "\n",
    "def calculate_metric_for_each_token(model, sequence, token_counts, metric, unsteered=True):\n",
    "    name_to_metric = {\n",
    "        \"avg_log_probs\": avg_lob_probs,\n",
    "        \"sequence_prob\": sequence_probability,\n",
    "        \"cross_entropy\": cross_entropy_loss,\n",
    "        \"perplexity\": calculate_perplexity,\n",
    "        \"perplexity_with_entropy\": calculate_perplexity_with_entropy,\n",
    "        \"perplexity_with_repetition_bias\": calculate_perplexity_with_repetition_bias,\n",
    "        \"entropy\": calculate_entropy,\n",
    "        \"repetition_score\": calculate_repetition_score,\n",
    "    }\n",
    "    if unsteered:\n",
    "        model.reset()\n",
    "\n",
    "    metric_values = []\n",
    "    metric_fn = name_to_metric[metric]\n",
    "\n",
    "    for i in token_counts:\n",
    "        metric_value = metric_fn(model, sequence[0:i])\n",
    "        metric_values.append(metric_value)\n",
    "    \n",
    "    return metric_values\n",
    "\n",
    "def calculate_metric_over_sequence(train_model, eval_model, tokenizer, input_text, vector, token_counts, start_coeff=0.16, iterations=20, end_coeff=1.0, repetition_penalty=1.0, temperature=1e-6, metric=\"norm_avg_log_probs\", unsteered=True):\n",
    "    coefficients = []\n",
    "    all_metric_values = []\n",
    "    outputs = []\n",
    "\n",
    "    coeff = start_coeff\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(train_model.device)\n",
    "    for _ in tqdm.tqdm(range(iterations), desc=\"Testing coefficients\"):\n",
    "        sequence = generate_sequence(train_model, input_ids, vector, coeff, token_counts[-1], repetition_penalty, temperature)\n",
    "        metric_values = calculate_metric_for_each_token(eval_model, sequence, token_counts, metric, unsteered=unsteered)\n",
    "\n",
    "        coefficients.append(coeff)\n",
    "        all_metric_values.append(metric_values)\n",
    "        output = tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "        outputs.append(output)\n",
    "        \n",
    "        coeff += (end_coeff-start_coeff)/(iterations-1)\n",
    "    \n",
    "    return coefficients, all_metric_values, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac5ee727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(positive_prompt, negative_prompt, dataset_size, method, layers, train_model, use_sae=True, k=100, use_residuals=True):\n",
    "    steering_dataset = make_dataset(\n",
    "        \"{persona}\",\n",
    "        [positive_prompt],\n",
    "        [negative_prompt],\n",
    "        \"\",\n",
    "        truncated_output_suffixes[:dataset_size],\n",
    "    )\n",
    "\n",
    "    print(f'Made dataset for prompt \"{positive_prompt}\", now training...')\n",
    "\n",
    "    train_model.reset()\n",
    "    if use_sae:\n",
    "        steering_vector = ControlVector.train_with_sae(\n",
    "            train_model,\n",
    "            tokenizer,\n",
    "            sae_full,\n",
    "            steering_dataset,\n",
    "            batch_size=32,\n",
    "            method=method,\n",
    "            hidden_layers=layers,\n",
    "            k=k,\n",
    "            use_residuals=use_residuals,\n",
    "        )\n",
    "    else:\n",
    "        steering_vector = ControlVector.train(\n",
    "            train_model,\n",
    "            tokenizer,\n",
    "            steering_dataset,\n",
    "            batch_size=32,\n",
    "            method=method,\n",
    "            hidden_layers=layers\n",
    "        )\n",
    "\n",
    "    print(f'Trained vector!')\n",
    "\n",
    "    return steering_vector\n",
    "\n",
    "def find_upper_bound(steering_vector, test_prompt, start_coeff, end_coeff, iterations, metric, token_counts, repetition_penalty, temperature, train_model, eval_model, unsteered):\n",
    "    coefficients, all_metric_values, outputs = calculate_metric_over_sequence(\n",
    "        train_model,\n",
    "        eval_model,\n",
    "        tokenizer,\n",
    "        test_prompt,\n",
    "        steering_vector,\n",
    "        token_counts,\n",
    "        start_coeff=start_coeff,\n",
    "        iterations=iterations,\n",
    "        end_coeff=end_coeff,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        temperature=temperature,\n",
    "        metric=metric,\n",
    "        unsteered=unsteered,\n",
    "    )\n",
    "\n",
    "    x = np.array(coefficients)\n",
    "    metric_values = [metrics[-1] for metrics in all_metric_values]\n",
    "\n",
    "    first_element = metric_values[0]\n",
    "    max_value = max(metric_values, key=lambda x: abs(first_element-x))\n",
    "    # Normalize values between -1 and 1, preserving signs\n",
    "    normalized_metric_values = []\n",
    "    for value in metric_values:\n",
    "        # Shift values so first_element becomes 0\n",
    "        shifted = value - first_element\n",
    "        # Scale by max absolute value to get range [-1,1]\n",
    "        if max_value != first_element:\n",
    "            normalized = abs(shifted / (max_value - first_element))\n",
    "        else:\n",
    "            # If all values are the same, return 0 to avoid division by zero\n",
    "            normalized = 0\n",
    "        normalized_metric_values.append(normalized)\n",
    "    \n",
    "    y = np.array(normalized_metric_values)\n",
    "\n",
    "    upper_bound_idx = find_significant_drop(x, y)\n",
    "\n",
    "    if upper_bound_idx:\n",
    "        upper_bound_idx -= 1\n",
    "        upper_bound = coefficients[upper_bound_idx]\n",
    "    else:\n",
    "        upper_bound = 1.0\n",
    "        upper_bound_idx = len(coefficients)-1\n",
    "    \n",
    "    # print(f\"Upper bound: {upper_bound}\")\n",
    "    # print(f\"Outputs near upper bound:\\n{outputs[max(0, upper_bound_idx-1)]}\\n{outputs[upper_bound_idx]}\\n{outputs[min(upper_bound_idx+1, len(outputs)-1)]}\")\n",
    "\n",
    "    # visualize_discontinuities(x, y, [upper_bound_idx])\n",
    "\n",
    "    return coefficients, all_metric_values, outputs, x, y, upper_bound_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d38e5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_discontinuities(x, ys, discontinuities, labels=None, plot_discontinuities=False):\n",
    "    \"\"\"\n",
    "    Visualize discontinuities for multiple y-series.\n",
    "    \n",
    "    Parameters:\n",
    "    x: array-like - x values\n",
    "    ys: list of array-like - multiple y-series to plot\n",
    "    discontinuities: list of int - discontinuity index for each y-series\n",
    "    labels: list of str - labels for each series (optional)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Generate colors for each series\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "    \n",
    "    # Use default labels if none provided\n",
    "    if labels is None:\n",
    "        labels = [f'Series {i+1}' for i in range(len(ys))]\n",
    "    \n",
    "    # Plot each y-series with its discontinuity\n",
    "    for i, (y, disc_idx, color, label) in enumerate(zip(ys, discontinuities, colors, labels)):\n",
    "        # Plot line\n",
    "        plt.plot(x, y, color=color, alpha=0.6, label=label)\n",
    "        \n",
    "        if plot_discontinuities:\n",
    "            # Plot discontinuity point\n",
    "            plt.scatter(x[disc_idx], y[disc_idx], c='r', s=100, \n",
    "                    zorder=3)\n",
    "\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title('Comparison of Metrics for Finding Maximum Activation Value for Steering Vector')\n",
    "    plt.xlabel('Coefficient')\n",
    "    plt.ylabel('Steering Effect Measurement Metric')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f7a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steer(positive_prompt, negative_prompt, test_prompt, layers, start_coeff, end_coeff, iterations, method, dataset_size, metric, token_counts, repetition_penalty, temperature, train_model, eval_model, use_sae=True): \n",
    "    steering_vector = train(positive_prompt, negative_prompt, dataset_size, method, layers, train_model, use_sae)\n",
    "    return find_upper_bound(steering_vector, test_prompt, start_coeff, end_coeff, iterations, metric, token_counts, repetition_penalty, temperature, train_model, eval_model, unsteered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e5f6baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made dataset for prompt \"I am talking about wedding ceremonies and traditions\", now training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating tokens: 100%|██████████| 74/74 [00:05<00:00, 13.89it/s]\n",
      "sae encoding: 100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "extracting directions: 100%|██████████| 30/30 [00:09<00:00,  3.17it/s]\n",
      "sae decoding: 100%|██████████| 30/30 [00:00<00:00, 536.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained vector!\n",
      "Training sae_topk_diff with k 10 on prompt \"talking about wedding ceremonies and traditions\" with test prompt \"talking about\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing coefficients:  50%|█████     | 30/60 [00:21<00:21,  1.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_prompt \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtalking about\u001b[39m\u001b[38;5;124m\"\u001b[39m, previous_prompt]:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# if train_prompt == test_prompt:\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m#     continue\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with k \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on prompt \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrain_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m with test prompt \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtest_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m     methods_results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfind_upper_bound\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteering_vector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI am \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtest_prompt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.125\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_log_probs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43munsteered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 40\u001b[0m, in \u001b[0;36mfind_upper_bound\u001b[0;34m(steering_vector, test_prompt, start_coeff, end_coeff, iterations, metric, token_counts, repetition_penalty, temperature, train_model, eval_model, unsteered)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_upper_bound\u001b[39m(steering_vector, test_prompt, start_coeff, end_coeff, iterations, metric, token_counts, repetition_penalty, temperature, train_model, eval_model, unsteered):\n\u001b[0;32m---> 40\u001b[0m     coefficients, all_metric_values, outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metric_over_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteering_vector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_coeff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_coeff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43munsteered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munsteered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(coefficients)\n\u001b[1;32m     57\u001b[0m     metric_values \u001b[38;5;241m=\u001b[39m [metrics[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m metrics \u001b[38;5;129;01min\u001b[39;00m all_metric_values]\n",
      "Cell \u001b[0;32mIn[10], line 221\u001b[0m, in \u001b[0;36mcalculate_metric_over_sequence\u001b[0;34m(train_model, eval_model, tokenizer, input_text, vector, token_counts, start_coeff, iterations, end_coeff, repetition_penalty, temperature, metric, unsteered)\u001b[0m\n\u001b[1;32m    219\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(train_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(iterations), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting coefficients\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 221\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoeff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m     metric_values \u001b[38;5;241m=\u001b[39m calculate_metric_for_each_token(eval_model, sequence, token_counts, metric, unsteered\u001b[38;5;241m=\u001b[39munsteered)\n\u001b[1;32m    224\u001b[0m     coefficients\u001b[38;5;241m.\u001b[39mappend(coeff)\n",
      "Cell \u001b[0;32mIn[10], line 187\u001b[0m, in \u001b[0;36mgenerate_sequence\u001b[0;34m(model, input_ids, vector, coeff, max_new_tokens, repetition_penalty, temperature)\u001b[0m\n\u001b[1;32m    180\u001b[0m settings \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad_token_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_new_tokens,\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepetition_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: repetition_penalty,\n\u001b[1;32m    185\u001b[0m }\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# <-- Add this line\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/workspace/steering-research/experiments/repeng/control.py:118\u001b[0m, in \u001b[0;36mControlModel.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:3274\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3271\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m next_token_logits\u001b[38;5;241m.\u001b[39mto(input_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3273\u001b[0m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[0;32m-> 3274\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3276\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[1;32m   3277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py:104\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py:532\u001b[0m, in \u001b[0;36mTopKLogitsWarper.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    530\u001b[0m top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Safety check\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# Remove all tokens with a probability less than the last token of the top-k\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m indices_to_remove \u001b[38;5;241m=\u001b[39m scores \u001b[38;5;241m<\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    533\u001b[0m scores_processed \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(indices_to_remove, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_value)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores_processed\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "methods_results = {}\n",
    "for method in [\"sae_topk_diff\",  \"sae_topk_center\"]:#, \"mean_center\", \"pca_diff\", \"pca_center\"]: #TODO: umap\n",
    "    for k in 10, 100, 1000, 10000:\n",
    "    # for use_sae in [True, False]:\n",
    "    #     if method in [\"sae_topk_diff\", \"sae_topk_center\"] and not use_sae:\n",
    "    #         continue\n",
    "        use_sae = True\n",
    "        prompts = [\n",
    "            # Descriptive/Conceptual Steering\n",
    "                # Descriptive/Conceptual Steering\n",
    "                # \"talking about the Golden Gate Bridge in San Francisco, California\",  # Concrete topic\n",
    "                # \"talking about quantum physics and its mathematical foundations\",     # Academic domain\n",
    "                # \"talking about environmental sustainability and climate change\",      # Complex concept\n",
    "                # \"talking about London and its historical landmarks\",                 # Location\n",
    "                \"talking about wedding ceremonies and traditions\",                   # Event\n",
    "                    \n",
    "                # Style/Voice Steering\n",
    "                \"speaking with empathy and emotional understanding\",                 # Emotional tone\n",
    "                # \"speaking in a technical and precise academic writing style\",              # Writing style\n",
    "                # \"speaking in a casual and friendly conversation style\",                    # Conversational style\n",
    "                # \"speaking with anger and frustration\",                    # Emotional state\n",
    "                # \"speaking with love and affection\",                       # Emotional state\n",
    "                # \"offering praise and appreciation\",                    # Positive tone\n",
    "                # \"discussing feelings of depression\",                   # Mental health\n",
    "                    \n",
    "                # Task/Format Steering\n",
    "                \"writing Python code with detailed comments\",          # Programming\n",
    "                # \"creating structured JSON data\",                       # Data format\n",
    "                # \"writing step-by-step tutorials\",                      # Instructional format\n",
    "                # \"writing in French\",                                   # Language\n",
    "                    \n",
    "                # Identity/Role Steering\n",
    "                # \"a helpful mathematics tutor\",                   # Expert role\n",
    "                # \"a creative storyteller\",                    # Creative role\n",
    "                # \"a Christian\",           # Religious perspective\n",
    "                \"a conspiracy theorist\",            # Critical analysis\n",
    "\n",
    "                #\"talking about\", #default\n",
    "            ]\n",
    "        for i, train_prompt in enumerate(prompts):\n",
    "            steering_vector = train(\n",
    "                positive_prompt=f\"I am {train_prompt}\",\n",
    "                negative_prompt=\"I am talking about whatever I want\",\n",
    "                dataset_size=4096,\n",
    "                method=method,\n",
    "                layers=list(range(1,31)),\n",
    "                train_model=base_model,\n",
    "                use_sae=use_sae,\n",
    "                k=k,\n",
    "            )\n",
    "\n",
    "            previous_prompt = prompts[i-1]\n",
    "            for test_prompt in [\"talking about\", previous_prompt]:\n",
    "                # if train_prompt == test_prompt:\n",
    "                #     continue\n",
    "                print(f\"Training {method} with k {k} on prompt \\\"{train_prompt}\\\" with test prompt \\\"{test_prompt}\\\"\")\n",
    "                methods_results[f\"{method}_{k}_{train_prompt}_{test_prompt}\"] = find_upper_bound(\n",
    "                    steering_vector,\n",
    "                    test_prompt=f\"I am {test_prompt}\",\n",
    "                    start_coeff=0,\n",
    "                    end_coeff=1.125,\n",
    "                    iterations=60,\n",
    "                    metric=\"avg_log_probs\",\n",
    "                    token_counts=[32],\n",
    "                    repetition_penalty=1.1,\n",
    "                    temperature=1e-6,\n",
    "                    train_model=base_model,\n",
    "                    eval_model=base_model,\n",
    "                    unsteered=True,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a81ddbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_text(criterion: str, prompt: str, completion: Union[str, List[str]], batch: bool = False, model: str = \"claude-3-5-sonnet-20241022\") -> Union[Dict, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Evaluate text using Claude API based on given criterion.\n",
    "    \n",
    "    Args:\n",
    "        criterion: Evaluation criterion to use\n",
    "        prompt: Original prompt given to the model\n",
    "        completion: Single completion string or list of completion strings\n",
    "        batch: Whether to process completions as a batch\n",
    "        \n",
    "    Returns:\n",
    "        dict or list[dict]: API response(s) containing scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format the system message\n",
    "    system_msg = f\"\"\"You score texts generated by a completion large language model based on the following criterion:\n",
    "{criterion}\n",
    "You provide a score from 1 to 10.\n",
    "Output format should be JSON with the following fields: \"score\" (int)\"\"\"\n",
    "\n",
    "    # Define the evaluation tool schema\n",
    "    tools = [{\n",
    "        \"name\": \"score_text\",\n",
    "        \"description\": \"Score the generated text based on the given criterion\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"score\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 1,\n",
    "                    \"maximum\": 10,\n",
    "                    \"description\": \"Score from 1-10 based on how well the text meets the criterion\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"score\"]\n",
    "        }\n",
    "    }]\n",
    "\n",
    "    # Initialize client\n",
    "    client = anthropic.Anthropic()\n",
    "\n",
    "    if not batch:\n",
    "        # Single completion processing\n",
    "        user_msg = f\"\"\"Completion:\n",
    "\\\"\\\"\\\"\n",
    "{completion}\n",
    "\\\"\\\"\\\"\"\"\"\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=1024,\n",
    "            system=system_msg,\n",
    "            temperature=0,\n",
    "            messages=[{\"role\": \"user\", \"content\": user_msg}],\n",
    "            tools=tools,\n",
    "            tool_choice={\"type\": \"tool\", \"name\": \"score_text\"}\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            for content in response.content:\n",
    "                if hasattr(content, 'type') and content.type == 'tool_use':\n",
    "                    return content.input\n",
    "            return {\"error\": \"No tool use found in response\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Failed to parse response: {str(e)}\", \"raw_response\": response.content}\n",
    "    \n",
    "    else:\n",
    "        # Batch processing\n",
    "        if not isinstance(completion, list):\n",
    "            completion = [completion]\n",
    "            \n",
    "        requests = []\n",
    "        for i, comp in enumerate(completion):\n",
    "            user_msg = f\"\"\"Completion:\n",
    "\\\"\\\"\\\"\n",
    "{comp}\n",
    "\\\"\\\"\\\"\"\"\"\n",
    "\n",
    "            requests.append(\n",
    "                Request(\n",
    "                    custom_id=f\"completion-{i}\",\n",
    "                    params=MessageCreateParamsNonStreaming(\n",
    "                        model=model,\n",
    "                        max_tokens=1024,\n",
    "                        system=system_msg,\n",
    "                        temperature=0,\n",
    "                        messages=[{\"role\": \"user\", \"content\": user_msg}],\n",
    "                        tools=tools,\n",
    "                        tool_choice={\"type\": \"tool\", \"name\": \"score_text\"}\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            return client.beta.messages.batches.create(requests=requests)\n",
    "\n",
    "        except Exception as e:\n",
    "            return [{\"error\": f\"Batch request failed: {str(e)}\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72d728fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_batch_ready(batch_id):\n",
    "    \"\"\"Check if a batch request is ready to be downloaded.\n",
    "    \n",
    "    Args:\n",
    "        batch_id: The ID of the batch request to check\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if batch is complete, False if still processing\n",
    "    \"\"\"\n",
    "    client = anthropic.Anthropic()\n",
    "    \n",
    "    try:\n",
    "        # Check first result to see if batch is complete\n",
    "        for result in client.beta.messages.batches.results(batch_id):\n",
    "            return result.result.type == \"succeeded\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking batch status: {str(e)}\")\n",
    "        return False\n",
    "        \n",
    "\n",
    "def get_batch_results(batch_id):\n",
    "    client = anthropic.Anthropic()\n",
    "\n",
    "    results = {}\n",
    "    # Stream results file in memory-efficient chunks, processing one at a time\n",
    "    for result in client.beta.messages.batches.results(\n",
    "        batch_id, \n",
    "    ):\n",
    "        match result.result.type:\n",
    "            case \"succeeded\":\n",
    "                try:\n",
    "                    for content in result.result.message.content:\n",
    "                        if hasattr(content, 'type') and content.type == 'tool_use':\n",
    "                            results[result.custom_id] = content.input\n",
    "                        else:\n",
    "                            print(f\"error: No tool use found in response: {result.custom_id}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to parse response: {str(e)}\")\n",
    "            case \"errored\":\n",
    "                if result.result.error.type == \"invalid_request\":\n",
    "                    # Request body must be fixed before re-sending request\n",
    "                    print(f\"Validation error {result.custom_id}\")\n",
    "                else:\n",
    "                    # Request can be retried directly\n",
    "                    print(f\"Server error {result.custom_id}\")\n",
    "            case \"expired\":\n",
    "                print(f\"Request expired {result.custom_id}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d916f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet_batches = {}\n",
    "\n",
    "for method in methods_results.keys():\n",
    "    sonnet_batches[method] = {}\n",
    "    prompt = method.split(\"_\")[-2]\n",
    "    sonnet_batches[method]['behavior'] = evaluate_text(\n",
    "        criterion=f\"How related is the text to {prompt}? Give extremely generous partial credit for subject matter that is even slightly, barely, tenuously, conceptually related. Evaluate relevance even if the text is not coherent.\",\n",
    "        prompt=\"\",\n",
    "        completion=methods_results[method][2],\n",
    "        batch=True\n",
    "    )\n",
    "    sonnet_batches[method]['coherence'] = evaluate_text(\n",
    "        criterion=\"The text makes sense and the grammar is correct. The text may be cut off mid-sentence; that should not affect your score.\",\n",
    "        prompt=\"\",\n",
    "        completion=methods_results[method][2],\n",
    "        batch=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9855d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "with open('sonnet_batches_topk.pkl', 'wb') as f:\n",
    "    pickle.dump(sonnet_batches, f)\n",
    "\n",
    "# Save to file\n",
    "with open('methods_results_topk.pkl', 'wb') as f:\n",
    "    pickle.dump(methods_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "359e4d4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'notebook_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load from file later\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mnotebook_dir\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautograding_batches/sonnet_batches_topk.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     loaded_batches \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(notebook_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautograding_results/methods_results_topk.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'notebook_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# Load from file later\n",
    "with open(os.path.join(notebook_dir, 'autograding_batches/sonnet_batches_topk.pkl'), 'rb') as f:\n",
    "    loaded_batches = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(notebook_dir, 'autograding_results/methods_results_topk.pkl'), 'rb') as f:\n",
    "    loaded_methods_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa747f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n",
      "Error: No `results_url` for the given batch; Has it finished processing? in_progress\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnthropicError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 9\u001b[0m\n\u001b[1;32m      8\u001b[0m batch_results[method] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 9\u001b[0m behavior_scores \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_batches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbehavior\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m coherence_scores \u001b[38;5;241m=\u001b[39m get_batch_results(loaded_batches[method][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoherence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mid)\n",
      "Cell \u001b[0;32mIn[53], line 26\u001b[0m, in \u001b[0;36mget_batch_results\u001b[0;34m(batch_id)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Stream results file in memory-efficient chunks, processing one at a time\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mmatch\u001b[39;00m result\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mtype:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/resources/beta/messages/batches.py:324\u001b[0m, in \u001b[0;36mBatches.results\u001b[0;34m(self, message_batch_id, betas, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mresults_url:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AnthropicError(\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo `results_url` for the given batch; Has it finished processing? \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;241m.\u001b[39mprocessing_status\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    328\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/binary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n",
      "\u001b[0;31mAnthropicError\u001b[0m: No `results_url` for the given batch; Has it finished processing? in_progress",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait 5 seconds before retrying\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batch_results = {}\n",
    "while True:\n",
    "        try:\n",
    "            batch_results = {}\n",
    "            for method in loaded_batches.keys():\n",
    "                batch_results[method] = {}\n",
    "                behavior_scores = get_batch_results(loaded_batches[method]['behavior'].id)\n",
    "                coherence_scores = get_batch_results(loaded_batches[method]['coherence'].id)\n",
    "                for i in range(len(loaded_methods_results[method][2])):\n",
    "                    batch_results[method][i] = {\n",
    "                        \"output\": loaded_methods_results[method][2][i],\n",
    "                        \"behavior\": behavior_scores[f\"completion-{i}\"]['score'],\n",
    "                        \"coherence\": coherence_scores[f\"completion-{i}\"]['score']\n",
    "                    }\n",
    "            print(\"done\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            time.sleep(5)  # Wait 5 seconds before retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc44656-17c4-4d5f-9a4d-4d83f409cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "with open('batch_results_topk.pkl', 'wb') as f:\n",
    "    pickle.dump(batch_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4927de99",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'autograding_results/batch_results_topk.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mautograding_results/batch_results_topk.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     batch_results \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/src/steering-research/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'autograding_results/batch_results_topk.pkl'"
     ]
    }
   ],
   "source": [
    "with open('autograding_results/batch_results_topk.pkl', 'rb') as f:\n",
    "    batch_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# First, let's organize the data by method (stripping prompts)\n",
    "method_averages_sae = defaultdict(list)\n",
    "method_averages_no_sae = defaultdict(list)\n",
    "\n",
    "for key, results in batch_results.items():\n",
    "    # Split the key to get method and prompt\n",
    "    parts = key.split('_')\n",
    "    # Find where the prompt starts\n",
    "    prompt_start = key.find('talking about')\n",
    "    if prompt_start == -1:\n",
    "        prompt_start = key.find('speaking with')\n",
    "    if prompt_start == -1:\n",
    "        prompt_start = key.find('writing Python')\n",
    "    if prompt_start == -1:\n",
    "        prompt_start = key.find('a conspiracy')\n",
    "        \n",
    "    base_method = key[:prompt_start-1]  # Get everything before the prompt\n",
    "    \n",
    "    # Calculate combined scores\n",
    "    indices = list(results.keys())\n",
    "    combined_scores = [results[i]['behavior'] * results[i]['coherence'] for i in indices]\n",
    "    \n",
    "    if '_no_sae' in base_method:\n",
    "        method_averages_no_sae[base_method].append(combined_scores)\n",
    "    else:\n",
    "        method_averages_sae[base_method].append(combined_scores)\n",
    "\n",
    "# Average across prompts for each method\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot SAE methods\n",
    "for method, scores_list in method_averages_sae.items():\n",
    "    avg_scores = np.mean(scores_list, axis=0)\n",
    "    plt.plot(range(len(avg_scores)), avg_scores, marker='o', label=method, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Average Combined Score (Behavior × Coherence)')\n",
    "plt.title('Combined Steering Performance by Method - SAE Methods')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "plt.ylim(0, 100)  # Set y-axis from 0 to 100\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot non-SAE methods\n",
    "plt.figure(figsize=(15, 10))\n",
    "for method, scores_list in method_averages_no_sae.items():\n",
    "    avg_scores = np.mean(scores_list, axis=0)\n",
    "    plt.plot(range(len(avg_scores)), avg_scores, marker='o', label=method, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Average Combined Score (Behavior × Coherence)')\n",
    "plt.title('Combined Steering Performance by Method - Non-SAE Methods')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "plt.ylim(0, 100)  # Set y-axis from 0 to 100\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now let's organize by prompt\n",
    "prompt_averages_sae = defaultdict(list)\n",
    "prompt_averages_no_sae = defaultdict(list)\n",
    "\n",
    "for key, results in batch_results.items():\n",
    "    # Split the key to get method and prompt\n",
    "    parts = key.split('_')\n",
    "    # Find where the prompt starts\n",
    "    prompt_start = key.find('talking about')\n",
    "    if prompt_start == -1:\n",
    "        prompt_start = key.find('speaking with')\n",
    "    if prompt_start == -1:\n",
    "        prompt_start = key.find('writing Python')\n",
    "    if prompt_start == -1:\n",
    "        prompt_start = key.find('a conspiracy')\n",
    "        \n",
    "    prompt = key[prompt_start:]\n",
    "    \n",
    "    # Calculate combined scores\n",
    "    indices = list(results.keys())\n",
    "    combined_scores = [results[i]['behavior'] * results[i]['coherence'] for i in indices]\n",
    "    \n",
    "    if '_no_sae' in key:\n",
    "        prompt_averages_no_sae[prompt].append(combined_scores)\n",
    "    else:\n",
    "        prompt_averages_sae[prompt].append(combined_scores)\n",
    "\n",
    "# Plot by prompt - SAE methods\n",
    "plt.figure(figsize=(15, 10))\n",
    "for prompt, scores_list in prompt_averages_sae.items():\n",
    "    avg_scores = np.mean(scores_list, axis=0)\n",
    "    plt.plot(range(len(avg_scores)), avg_scores, marker='o', label=prompt, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Average Combined Score (Behavior × Coherence)')\n",
    "plt.title('Combined Steering Performance by Prompt - SAE Methods')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "plt.ylim(0, 100)  # Set y-axis from 0 to 100\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot by prompt - non-SAE methods\n",
    "plt.figure(figsize=(15, 10))\n",
    "for prompt, scores_list in prompt_averages_no_sae.items():\n",
    "    avg_scores = np.mean(scores_list, axis=0)\n",
    "    plt.plot(range(len(avg_scores)), avg_scores, marker='o', label=prompt, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Average Combined Score (Behavior × Coherence)')\n",
    "plt.title('Combined Steering Performance by Prompt - Non-SAE Methods')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "plt.ylim(0, 100)  # Set y-axis from 0 to 100\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create separate dictionaries for coherence and behavior\n",
    "method_averages_sae_coherence = defaultdict(list)\n",
    "method_averages_no_sae_coherence = defaultdict(list)\n",
    "method_averages_sae_behavior = defaultdict(list)\n",
    "method_averages_no_sae_behavior = defaultdict(list)\n",
    "\n",
    "# Process methods\n",
    "for key, results in batch_results.items():\n",
    "    # Split the key to get method and prompt\n",
    "    parts = key.split('_')\n",
    "    # Find where the prompt starts\n",
    "    prompt_start = key.find('talking about')\n",
    "    if prompt_start == -1:\n",
    "        prompt_start = key.find('speaking with')\n",
    "    if prompt_start == -1:\n",
    "        prompt_start = key.find('writing Python')\n",
    "    if prompt_start == -1:\n",
    "        prompt_start = key.find('a conspiracy')\n",
    "        \n",
    "    base_method = key[:prompt_start-1]  # Get everything before the prompt\n",
    "    \n",
    "    # Calculate separate scores\n",
    "    indices = list(results.keys())\n",
    "    coherence_scores = [results[i]['coherence'] for i in indices]\n",
    "    behavior_scores = [results[i]['behavior'] for i in indices]\n",
    "    \n",
    "    if '_no_sae' in base_method:\n",
    "        method_averages_no_sae_coherence[base_method].append(coherence_scores)\n",
    "        method_averages_no_sae_behavior[base_method].append(behavior_scores)\n",
    "    else:\n",
    "        method_averages_sae_coherence[base_method].append(coherence_scores)\n",
    "        method_averages_sae_behavior[base_method].append(behavior_scores)\n",
    "\n",
    "# Create plots for methods\n",
    "def plot_method_scores(averages_dict, title):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for method, scores_list in averages_dict.items():\n",
    "        avg_scores = np.mean(scores_list, axis=0)\n",
    "        plt.plot(range(len(avg_scores)), avg_scores, marker='o', label=method, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Coefficient Index')\n",
    "    plt.ylabel('Average Score')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    plt.ylim(0, 10)  # Set y-axis from 0 to 10\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot all method combinations\n",
    "plot_method_scores(method_averages_sae_coherence, 'Coherence Performance by Method - SAE Methods')\n",
    "plot_method_scores(method_averages_no_sae_coherence, 'Coherence Performance by Method - Non-SAE Methods')\n",
    "plot_method_scores(method_averages_sae_behavior, 'Behavior Performance by Method - SAE Methods')\n",
    "plot_method_scores(method_averages_no_sae_behavior, 'Behavior Performance by Method - Non-SAE Methods')\n",
    "\n",
    "# Now process by prompt\n",
    "prompt_averages_sae_coherence = defaultdict(list)\n",
    "prompt_averages_no_sae_coherence = defaultdict(list)\n",
    "prompt_averages_sae_behavior = defaultdict(list)\n",
    "prompt_averages_no_sae_behavior = defaultdict(list)\n",
    "\n",
    "for key, results in batch_results.items():\n",
    "    # Split the key to get method and prompt\n",
    "    parts = key.split('_')\n",
    "    # Find where the prompt starts\n",
    "    prompt_start = key.find('talking about')\n",
    "    if prompt_start == -1:\n",
    "        prompt_start = key.find('speaking with')\n",
    "    if prompt_start == -1:\n",
    "        prompt_start = key.find('writing Python')\n",
    "    if prompt_start == -1:\n",
    "        prompt_start = key.find('a conspiracy')\n",
    "        \n",
    "    prompt = key[prompt_start:]\n",
    "    \n",
    "    # Calculate separate scores\n",
    "    indices = list(results.keys())\n",
    "    coherence_scores = [results[i]['coherence'] for i in indices]\n",
    "    behavior_scores = [results[i]['behavior'] for i in indices]\n",
    "    \n",
    "    if '_no_sae' in key:\n",
    "        prompt_averages_no_sae_coherence[prompt].append(coherence_scores)\n",
    "        prompt_averages_no_sae_behavior[prompt].append(behavior_scores)\n",
    "    else:\n",
    "        prompt_averages_sae_coherence[prompt].append(coherence_scores)\n",
    "        prompt_averages_sae_behavior[prompt].append(behavior_scores)\n",
    "\n",
    "# Plot all prompt combinations\n",
    "plot_method_scores(prompt_averages_sae_coherence, 'Coherence Performance by Prompt - SAE Methods')\n",
    "plot_method_scores(prompt_averages_no_sae_coherence, 'Coherence Performance by Prompt - Non-SAE Methods')\n",
    "plot_method_scores(prompt_averages_sae_behavior, 'Behavior Performance by Prompt - SAE Methods')\n",
    "plot_method_scores(prompt_averages_no_sae_behavior, 'Behavior Performance by Prompt - Non-SAE Methods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d0418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_series(coefficients, values):\n",
    "    \"\"\"Normalize a single series by its peak\"\"\"\n",
    "    peak_idx = np.argmax(values)\n",
    "    # Normalize x axis by peak location\n",
    "    x = np.array(coefficients) - coefficients[peak_idx]\n",
    "\n",
    "    return x\n",
    "\n",
    "def plot_aggregated_results(data_dict, title, xlabel=\"Distance from Peak\", ylabel=\"Normalized Score\"):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for label, series in data_dict.items():\n",
    "        # Get all x and y values for this group\n",
    "        x_values = [s[0] for s in series]  # List of x arrays\n",
    "        y_values = [s[1] for s in series]  # List of y arrays\n",
    "        \n",
    "        # Find common x range\n",
    "        min_x = min(np.min(x) for x in x_values)\n",
    "        max_x = max(np.max(x) for x in x_values)\n",
    "        common_x = np.arange(min_x, max_x + 1)\n",
    "        \n",
    "        # For each series, get y values at common x points\n",
    "        aligned_y = []\n",
    "        for x, y in zip(x_values, y_values):\n",
    "            # Find where this series' x values match common_x\n",
    "            mask = np.isin(common_x, x)\n",
    "            aligned = np.zeros(len(common_x))\n",
    "            aligned[mask] = y[np.isin(x, common_x)]\n",
    "            aligned_y.append(aligned)\n",
    "        \n",
    "        # Now we can safely average\n",
    "        avg_scores = np.mean(aligned_y, axis=0)\n",
    "        std_scores = np.std(aligned_y, axis=0)\n",
    "        \n",
    "        plt.plot(common_x, avg_scores, marker='o', label=label, alpha=0.7)\n",
    "        plt.fill_between(common_x, avg_scores-std_scores, avg_scores+std_scores, alpha=0.2)\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add vertical line at x=0 (peak location)\n",
    "    plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Helper function to parse method string\n",
    "def parse_method_string(method_str):\n",
    "    sae_type = 'no_sae' if '_no_sae_' in method_str else 'sae'\n",
    "    \n",
    "    # Find indices of train and test prompts\n",
    "    sae_start = method_str.find('_sae_' if sae_type == 'sae' else '_no_sae_')\n",
    "    base_method = method_str[:sae_start]\n",
    "    train_start = sae_start + len('_sae_' if sae_type == 'sae' else '_no_sae_')\n",
    "    train_prompt = method_str[train_start:].rsplit('_', 1)[0]\n",
    "    test_prompt = method_str[train_start:].rsplit('_', 1)[1]\n",
    "    \n",
    "    return base_method, sae_type, train_prompt, test_prompt\n",
    "\n",
    "# Initialize aggregation dictionaries\n",
    "sae_vs_nonsae = defaultdict(list)\n",
    "by_method = defaultdict(list)\n",
    "by_method_sae = defaultdict(list)\n",
    "by_method_no_sae = defaultdict(list)\n",
    "by_train_prompt = defaultdict(list)\n",
    "by_train_test_prompt = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Process results\n",
    "for method, results in batch_results.items():\n",
    "    # Calculate combined scores\n",
    "    indices = list(results.keys())\n",
    "    coefficients = np.array(indices)\n",
    "    combined_scores = np.array([results[i]['behavior'] * results[i]['coherence'] for i in indices])\n",
    "    \n",
    "    # Normalize this series\n",
    "    norm_x = normalize_series(coefficients, combined_scores)\n",
    "    \n",
    "    # Parse method components\n",
    "    base_method, sae_type, train_prompt, test_prompt = parse_method_string(method)\n",
    "    \n",
    "    # Aggregate for each view using normalized values\n",
    "    sae_vs_nonsae[sae_type].append((norm_x, combined_scores))\n",
    "    by_method[base_method].append((norm_x, combined_scores))\n",
    "    \n",
    "    # Split methods by SAE/non-SAE\n",
    "    if sae_type == 'sae':\n",
    "        by_method_sae[f\"{base_method}\"].append((norm_x, combined_scores))\n",
    "    else:\n",
    "        by_method_no_sae[f\"{base_method}\"].append((norm_x, combined_scores))\n",
    "        \n",
    "    by_train_prompt[train_prompt].append((norm_x, combined_scores))\n",
    "    by_train_test_prompt[train_prompt][test_prompt].append((norm_x, combined_scores))\n",
    "\n",
    "# Plot each aggregation\n",
    "plot_aggregated_results(sae_vs_nonsae, \"SAE vs Non-SAE Performance\")\n",
    "plot_aggregated_results(by_method, \"Performance by Base Method\")\n",
    "plot_aggregated_results(by_method_sae, \"Performance by Method - SAE only\")\n",
    "plot_aggregated_results(by_method_no_sae, \"Performance by Method - Non-SAE only\")\n",
    "plot_aggregated_results(by_train_prompt, \"Performance by Training Prompt\")\n",
    "\n",
    "# Plot train-test prompt relationships\n",
    "for train_prompt, test_results in by_train_test_prompt.items():\n",
    "    plot_aggregated_results(\n",
    "        test_results, \n",
    "        f\"Performance for Training Prompt: {train_prompt}\"\n",
    "    )\n",
    "\n",
    "# Create method-specific SAE comparisons\n",
    "for method in set(by_method.keys()):\n",
    "    sae_comparison = {\n",
    "        'sae': by_method_sae.get(method, []),\n",
    "        'no_sae': by_method_no_sae.get(method, [])\n",
    "    }\n",
    "    # Only plot if we have data for both SAE/no-SAE\n",
    "    if sae_comparison['sae'] and sae_comparison['no_sae']:\n",
    "        plot_aggregated_results(\n",
    "            sae_comparison, \n",
    "            f\"SAE vs Non-SAE Performance - {method} Method\",\n",
    "            ylabel=f\"Score ({method})\"\n",
    "        )\n",
    "\n",
    "# Create comparison of all no_sae methods plus sae_topk\n",
    "comparison_dict = {}\n",
    "\n",
    "# Add all no_sae methods\n",
    "for method in by_method_no_sae.keys():\n",
    "    comparison_dict[f\"{method}_no_sae\"] = by_method_no_sae[method]\n",
    "\n",
    "# Add sae_topk\n",
    "if 'sae_topk_diff' in by_method_sae:\n",
    "    comparison_dict['sae_topk_diff'] = by_method_sae['sae_topk_diff']\n",
    "if 'sae_topk_center' in by_method_sae:\n",
    "    comparison_dict['sae_topk_center'] = by_method_sae['sae_topk_center']\n",
    "\n",
    "# Create the plot\n",
    "plot_aggregated_results(\n",
    "    comparison_dict,\n",
    "    \"No-SAE Methods vs SAE TopK\",\n",
    "    ylabel=\"Score\"\n",
    ")\n",
    "\n",
    "# Create prompt-specific method comparisons\n",
    "for train_prompt in set(by_train_prompt.keys()):\n",
    "    # Create a dictionary for this prompt's methods\n",
    "    prompt_methods = defaultdict(list)\n",
    "    \n",
    "    # Process all results for this prompt\n",
    "    for method, results in batch_results.items():\n",
    "        # Parse method components\n",
    "        base_method, sae_type, this_train_prompt, test_prompt = parse_method_string(method)\n",
    "        \n",
    "        # Only process if it matches our current train_prompt\n",
    "        if this_train_prompt == train_prompt:\n",
    "            # Calculate combined scores\n",
    "            indices = list(results.keys())\n",
    "            coefficients = np.array(indices)\n",
    "            combined_scores = np.array([results[i]['behavior'] * results[i]['coherence'] for i in indices])\n",
    "            \n",
    "            # Normalize this series\n",
    "            norm_x = normalize_series(coefficients, combined_scores)\n",
    "            \n",
    "            # Add to prompt methods dictionary\n",
    "            method_name = f\"{base_method}_{sae_type}\"\n",
    "            prompt_methods[method_name].append((norm_x, combined_scores))\n",
    "    \n",
    "    # Create plot if we have data\n",
    "    if prompt_methods:\n",
    "        plot_aggregated_results(\n",
    "            prompt_methods,\n",
    "            f\"Method Comparison for Prompt: {train_prompt}\",\n",
    "            ylabel=\"Score\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
