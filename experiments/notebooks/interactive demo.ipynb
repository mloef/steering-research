{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46f10d-891a-4162-99be-1b381498457a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Notebook / Runpod setup, safe to ignore\n",
    "%pip install matplotlib accelerate datasets einops huggingface-hub jaxtyping natsort simple-parsing triton transformers gguf sentencepiece scikit-learn seaborn\n",
    "%pip install -U safetensors\n",
    "%pip install git+https://github.com/EleutherAI/sae.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0074cbe-195f-43f3-94dd-2a049d99bbbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec2cea5e-f73c-4c67-a3e1-89305878adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "notebook_dir = os.path.abspath('')  # Get current notebook directory\n",
    "experiments_dir = os.path.dirname(notebook_dir)  # Get parent directory\n",
    "sys.path.insert(0, experiments_dir)\n",
    "os.environ[\"HF_HOME\"] = os.path.join(experiments_dir, \"hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676136dc-cd1e-4448-941c-8dd0e5237f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be85a3ae-6c14-43b1-8fe2-a98ba9cb0e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from repeng import ControlVector, ControlModel, DatasetEntry\n",
    "import repeng.saes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc143c3-73d3-43bb-8db1-ef201e06a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Collect garbage\n",
    "gc.collect()\n",
    "\n",
    "# Force CUDA to sync\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "control_layers = list(range(2, 30))\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "    torch_dtype=\"auto\").to(\n",
    "    torch.device(\"cuda:0\"))\n",
    "base_model = ControlModel(base_model, control_layers)\n",
    "\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    torch_dtype=\"auto\").to(\n",
    "    torch.device(\"cuda:0\"))\n",
    "instruct_model = ControlModel(instruct_model, control_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe04ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = repeng.saes.from_eleuther(device=\"cuda:0\", layers=control_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import textwrap\n",
    "\n",
    "with open(os.path.join(notebook_dir, \"data/all_truncated_outputs.json\")) as f:\n",
    "    output_suffixes = json.load(f)\n",
    "truncated_output_suffixes = [\n",
    "    tokenizer.convert_tokens_to_string(tokens[:i])\n",
    "    for tokens in (tokenizer.tokenize(s) for s in output_suffixes)\n",
    "    for i in range(1, len(tokens))\n",
    "]\n",
    "\n",
    "def visualize_discontinuities(x, y, discontinuities):\n",
    "    \"\"\"\n",
    "    Visualize the detected discontinuities on the original data.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Change from line plot to scatter plot\n",
    "    plt.scatter(x, y, c='b', s=20, alpha=0.6, label='Original data')\n",
    "    \n",
    "    x_discontinuities = x[discontinuities]\n",
    "    y_discontinuities = y[discontinuities]\n",
    "\n",
    "    plt.scatter(x_discontinuities, y_discontinuities, c='r', s=100, \n",
    "               label='Detected discontinuities', zorder=3)\n",
    "    \n",
    "    # Add vertical lines at discontinuities\n",
    "    for idx in discontinuities:\n",
    "        plt.axvline(x=x[idx], color='r', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title('Detected Discontinuities')\n",
    "    plt.show()\n",
    "\n",
    "def find_significant_drop(x, y, drop_threshold=0.2, window_size=1):\n",
    "    \"\"\"\n",
    "    Find the first point where perplexity drops significantly below starting value.\n",
    "    \n",
    "    Parameters:\n",
    "    x: array-like, coefficients\n",
    "    y: array-like, perplexity values\n",
    "    drop_threshold: float, minimum drop as fraction of starting value (default 0.2 = 20%)\n",
    "    window_size: int, number of consecutive points to check to avoid noise (default 3)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (x value where drop occurs, index of drop point)\n",
    "    \"\"\"\n",
    "    # Get baseline from start of sequence\n",
    "    baseline = y[0]\n",
    "    target_value = baseline * (1 - drop_threshold)\n",
    "    \n",
    "    # Look for first window where all values are below target\n",
    "    for i in range(len(y) - window_size + 1):\n",
    "        window = y[i:i+window_size]\n",
    "        if all([val < target_value for val in window]):\n",
    "            return i\n",
    "            \n",
    "    # If no significant drop found\n",
    "    return None\n",
    "\n",
    "\n",
    "def calculate_sequence_probability(model, sequence):\n",
    "    input_ids = sequence.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    sequence_prob = 0.0  # Start with 0 since we're adding logs\n",
    "    for i in range(input_ids.size(1)):\n",
    "        token_id = input_ids[0, i].item()\n",
    "        token_log_prob = log_probs[0, i, token_id].item()\n",
    "        sequence_prob += token_log_prob  # Add instead of multiply\n",
    "    \n",
    "    return sequence_prob/input_ids.size(1)  # This will be the avg log probability\n",
    "\n",
    "def generate_sequence(model, input_ids, vector, coeff, max_new_tokens):\n",
    "    model.reset()\n",
    "    model.set_control(coeff * vector)\n",
    "    settings = {\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"temperature\": 1e-6,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        #\"repetition_penalty\": 1,\n",
    "    }\n",
    "    with torch.no_grad():  # <-- Add this line\n",
    "        output = model.generate(**input_ids, **settings)\n",
    "    return output[0]\n",
    "\n",
    "def calculate_perplexity_for_each_token(model, sequence):\n",
    "    model.reset()\n",
    "    perplexities = []\n",
    "    for i in range(1, len(sequence)+1):\n",
    "        #pplx = calculate_perplexity(model, sequence[0:i])\n",
    "        pplx = calculate_sequence_probability(model, sequence[0:i])\n",
    "        #pplx = calculate_adjusted_perplexity(pplx, current_text)\n",
    "        perplexities.append(pplx)\n",
    "    return perplexities\n",
    "\n",
    "def calculate_perplexities_over_sequence(model, tokenizer, input_text, vector, token_count, start_coeff=0.16, iterations=20, end_coeff=1.0):\n",
    "    coefficients = []\n",
    "    all_perplexities = []\n",
    "    outputs = []\n",
    "\n",
    "    coeff = start_coeff\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    for _ in tqdm.tqdm(range(iterations), desc=\"Testing coefficients\"):\n",
    "        #print(coeff)\n",
    "        sequence = generate_sequence(model, input_ids, vector, coeff, token_count)\n",
    "        perplexities = calculate_perplexity_for_each_token(model, sequence)\n",
    "        \n",
    "        coefficients.append(coeff)\n",
    "        all_perplexities.append(perplexities)\n",
    "        output = tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "        outputs.append(output)\n",
    "        \n",
    "        coeff += (end_coeff-start_coeff)/(iterations-1)\n",
    "    \n",
    "    return coefficients, all_perplexities, outputs\n",
    "\n",
    "TEMPLATE = \"\"\"{persona}. {prefill}\"\"\"\n",
    "\n",
    "def make_dataset(\n",
    "    persona_template: str,\n",
    "    positive_personas: list[str],\n",
    "    negative_personas: list[str],\n",
    "    user_msg: str,\n",
    "    suffix_list: list[str]\n",
    ") -> list[DatasetEntry]:\n",
    "    dataset = []\n",
    "    for suffix in suffix_list:\n",
    "        for positive_persona, negative_persona in zip(positive_personas, negative_personas):\n",
    "            pos = persona_template.format(persona=positive_persona)\n",
    "            neg = persona_template.format(persona=negative_persona)\n",
    "            dataset.append(\n",
    "                DatasetEntry(\n",
    "                    positive=TEMPLATE.format(persona=pos, user_msg=user_msg, prefill=suffix),\n",
    "                    negative=TEMPLATE.format(persona=neg, user_msg=user_msg, prefill=suffix),\n",
    "                )\n",
    "            )\n",
    "    return dataset\n",
    "\n",
    "# Create all widgets\n",
    "mode_toggle = widgets.ToggleButtons(\n",
    "    options=['Base Mode', 'Chat Mode'],\n",
    "    value='Base Mode'\n",
    ")\n",
    "\n",
    "steering_input = widgets.Textarea(\n",
    "    placeholder='Enter steering prompt...',\n",
    "    layout={'width': '100%'}\n",
    ")\n",
    "\n",
    "input_box = widgets.Textarea(\n",
    "    placeholder='Enter text...',\n",
    "    layout={'width': '100%'}\n",
    ")\n",
    "\n",
    "upper_bound = 2.0\n",
    "\n",
    "strength_slider = widgets.FloatSlider(\n",
    "    min=-1 * upper_bound, max=upper_bound, value=0.0,\n",
    "    description='Strength:',\n",
    "    step=0.01,\n",
    "    #Add marks at key points\n",
    "    # marks=[\n",
    "    #     (-1 * upper_bound, '-max'),\n",
    "    #     (upper_bound, 'max')\n",
    "    # ],\n",
    "    # Make marks visible\n",
    "    readout=True,\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(description='Generate')\n",
    "\n",
    "steering_generate_button = widgets.Button(description='Train Steering Vector')\n",
    "\n",
    "clear_history_button = widgets.Button(description='Clear History')\n",
    "\n",
    "status_display = widgets.HTML()\n",
    "output_display = widgets.Output()\n",
    "\n",
    "# Main container for chat history\n",
    "chat_display = widgets.HTML()\n",
    "chat_history = []\n",
    "\n",
    "steering_vector = None\n",
    "\n",
    "def on_generate_click(b):\n",
    "    if mode_toggle.value == 'Base Mode':\n",
    "        # Handle base mode generation\n",
    "        prompt = input_box.value.strip()\n",
    "        if not prompt:\n",
    "            status_display.value = 'Please enter text'\n",
    "            return\n",
    "        if not steering_vector:\n",
    "            status_display.value = 'Please generate a steering vector first'\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            status_display.value = f'Generating from base model with prompt \"{prompt}\" and coefficient {strength_slider.value:.2f}. Please wait...'\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "            base_model.reset()\n",
    "            base_model.set_control(strength_slider.value * steering_vector)\n",
    "            settings = {\n",
    "                \"pad_token_id\": tokenizer.eos_token_id,\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_new_tokens\": 128,\n",
    "                \"repetition_penalty\": 1.1,\n",
    "            }\n",
    "            with torch.no_grad():  # <-- Add this line\n",
    "                sequence = base_model.generate(**input_ids, **settings)[0]\n",
    "            output = tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "            \n",
    "            status_display.value = textwrap.fill(output, width=80)\n",
    "                \n",
    "        except Exception as e:\n",
    "            status_display.value = f'Error: {str(e)}'\n",
    "            \n",
    "    else:\n",
    "        # Handle chat mode\n",
    "        message = input_box.value.strip()\n",
    "        if not message:\n",
    "            status_display.value = 'Please enter a message'\n",
    "            return\n",
    "            \n",
    "        # Add user message to history\n",
    "        chat_history.append(('user', message))\n",
    "        input_box.value = ''\n",
    "        \n",
    "        try:\n",
    "            status_display.value = f'Generating from instruct model with prompt \"{message}\" and coefficient {strength_slider.value:.2f}. Please wait...'\n",
    "            # Format the full chat history into a single prompt\n",
    "            chat_prompt = \"\"\n",
    "            \n",
    "            for entry in chat_history:\n",
    "                role, content = entry\n",
    "                if role == \"user\":\n",
    "                    chat_prompt += f\"<|start_header_id|>user<|end_header_id|>\\n{content}<|eot_id|>\\n\"\n",
    "                else:\n",
    "                    chat_prompt += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{content}<|eot_id|>\\n\"\n",
    "            \n",
    "            chat_prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "            # Generate response using the same logic as base mode\n",
    "            input_ids = tokenizer(chat_prompt, return_tensors=\"pt\").to(instruct_model.device)\n",
    "            instruct_model.reset()\n",
    "            instruct_model.set_control(strength_slider.value * steering_vector)\n",
    "            \n",
    "            settings = {\n",
    "                \"pad_token_id\": tokenizer.eos_token_id,\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_new_tokens\": 128,\n",
    "                \"repetition_penalty\": 1.1,\n",
    "            }\n",
    "            \n",
    "            with torch.no_grad():  # <-- Add this line\n",
    "                output = instruct_model.generate(\n",
    "                    **input_ids, \n",
    "                    **settings\n",
    "                )\n",
    "            response = tokenizer.decode(output[0])\n",
    "\n",
    "            # Split to get just the assistant's response\n",
    "            response = response.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1]\n",
    "\n",
    "            # No need to split on eot_id since it's at the end\n",
    "            response = response.strip()\n",
    "            if response.endswith(\"<|eot_id|>\"):\n",
    "                response = response[:-len(\"<|eot_id|>\")]\n",
    "\n",
    "            # Add response to chat history\n",
    "            chat_history.append(('assistant', response))\n",
    "            \n",
    "            # Update chat display\n",
    "            html = []\n",
    "            for role, content in chat_history:\n",
    "                color = \"blue\" if role == \"user\" else \"green\"\n",
    "                html.append(f'<div style=\"color: {color}\"><b>{role}:</b> {content}</div>')\n",
    "            chat_display.value = '<br>'.join(html)\n",
    "\n",
    "            status_display.value = \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            status_display.value = f'Error: {str(e)}'\n",
    "            # You can access chat_history for context\n",
    "            chat_history.pop()\n",
    "            \n",
    "            # Update chat display\n",
    "            html = []\n",
    "            for role, content in chat_history:\n",
    "                color = \"blue\" if role == \"user\" else \"green\"\n",
    "                html.append(f'<div style=\"color: {color}\"><b>{role}:</b> {content}</div>')\n",
    "            chat_display.value = '<br>'.join(html)\n",
    "            \n",
    "\n",
    "def on_mode_change(change):\n",
    "    if change['new'] == 'Base Mode':\n",
    "        chat_display.layout.display = 'none'\n",
    "        output_display.layout.display = 'block'\n",
    "    else:\n",
    "        chat_display.layout.display = 'block'\n",
    "        output_display.layout.display = 'none'\n",
    "    input_box.value = ''\n",
    "    status_display.value = ''\n",
    "\n",
    "def on_steering_generate_click(b):\n",
    "    #setattr(status_display, 'value', 'Steering vector generated!')\n",
    "    steering_prompt = steering_input.value.strip()\n",
    "    if not steering_prompt:\n",
    "        status_display.value = 'Please enter steering prompt'\n",
    "        return\n",
    "    \n",
    "    steering_dataset = make_dataset(\n",
    "        \"{persona}\",\n",
    "        [steering_prompt],\n",
    "        [\"an AI\"],\n",
    "        \"\",\n",
    "        truncated_output_suffixes,\n",
    "    )\n",
    "\n",
    "    status_display.value = f'Made dataset for prompt \"{steering_prompt}\", now training...'\n",
    "\n",
    "    base_model.reset()\n",
    "    global steering_vector\n",
    "    steering_vector = ControlVector.train_with_sae(\n",
    "        base_model,\n",
    "        tokenizer,\n",
    "        sae,\n",
    "        steering_dataset,\n",
    "        batch_size=32,\n",
    "        method=\"pca_center\",\n",
    "        hidden_layers=control_layers\n",
    "    )\n",
    "\n",
    "    status_display.value = 'Trained, now calculating maximum steering coefficient...'\n",
    "\n",
    "    input_text = \"I am\"\n",
    "    token_count = 24\n",
    "\n",
    "    #print(f\"Calculating for {token_count} tokens...\")\n",
    "    coefficients, all_perplexities, _ = calculate_perplexities_over_sequence(base_model, tokenizer, input_text, steering_vector, token_count, start_coeff=0., iterations=10, end_coeff=1.0)\n",
    "    x = np.array(coefficients)\n",
    "    perplexities = [pplx[-1] for pplx in all_perplexities]\n",
    "    normalized_perplexities = [pplx / perplexities[0] for pplx in perplexities]\n",
    "    y = np.array(normalized_perplexities)\n",
    "\n",
    "    upper_bound_idx = find_significant_drop(x, y)\n",
    "    global upper_bound\n",
    "    if upper_bound_idx:\n",
    "        upper_bound = coefficients[upper_bound_idx-1]\n",
    "    else:\n",
    "        upper_bound = 1.0\n",
    "        upper_bound_idx = len(coefficients)-1\n",
    "    # After calculating the new upper_bound\n",
    "    strength_slider.min = -1 * upper_bound\n",
    "    strength_slider.max = upper_bound\n",
    "    # Optionally reset value to 0 or clamp to new range\n",
    "    strength_slider.value = max(min(strength_slider.value, upper_bound), -1 * upper_bound)\n",
    "\n",
    "    status_display.value = 'Recommended min/max coefficients are -{} and {}. Ready to steer!'.format(upper_bound, upper_bound)\n",
    "\n",
    "    visualize_discontinuities(x, y, [upper_bound_idx])\n",
    "\n",
    "def on_clear_history(b):\n",
    "    global chat_history\n",
    "    chat_history = []\n",
    "    chat_display.value = \"\"\n",
    "    status_display.value = \"\"\n",
    "\n",
    "def on_mode_change(change):\n",
    "    if change['new'] == 'Base Mode':\n",
    "        chat_display.layout.display = 'none'\n",
    "        clear_history_button.layout.display = 'none'  # Hide clear button in base mode\n",
    "        output_display.layout.display = 'block'\n",
    "    else:\n",
    "        chat_display.layout.display = 'block'\n",
    "        clear_history_button.layout.display = 'block'  # Show clear button in chat mode\n",
    "        output_display.layout.display = 'none'\n",
    "    input_box.value = ''\n",
    "    status_display.value = ''\n",
    "\n",
    "# Wire up callbacks\n",
    "generate_button.on_click(on_generate_click)\n",
    "steering_generate_button.on_click(on_steering_generate_click)\n",
    "mode_toggle.observe(on_mode_change, names='value')\n",
    "clear_history_button.on_click(on_clear_history)\n",
    "\n",
    "# Create and display interface\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Text Generation Demo</h3>\"),\n",
    "    mode_toggle,\n",
    "    steering_input,\n",
    "    steering_generate_button,\n",
    "    input_box,\n",
    "    strength_slider,\n",
    "    generate_button,\n",
    "    clear_history_button,\n",
    "    status_display,\n",
    "    output_display,\n",
    "    chat_display\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
